{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.81075931e-03  9.45776701e-05]\n",
      " [ 1.66058540e-04 -7.13914633e-05]\n",
      " [ 4.79243696e-03  1.52818859e-03]\n",
      " [-9.89729300e-01  9.98275355e-01]]\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create a transformers pipeline for sentiment analysis\n",
    "sentiment_pipeline = pipeline('sentiment-analysis')\n",
    "\n",
    "# Create a text masker\n",
    "masker = shap.maskers.Text(mask_token=\"[MASKMASKMASK]\")\n",
    "\n",
    "# Create an explainer with the pipeline and masker\n",
    "explainer = shap.Explainer(sentiment_pipeline, masker)\n",
    "\n",
    "# Create an input text\n",
    "input_text = \"The cat is cute\"\n",
    "\n",
    "# Calculate the SHAP values\n",
    "shap_values = explainer([input_text])\n",
    "\n",
    "# Print the SHAP values\n",
    "print(shap_values[0].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Ġcat', 'Ġis', 'Ġso', 'Ġvery', 'Ġvery', 'Ġcute']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "tokenizer.tokenize(\"The cat is so very very cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 133, 4758, 16, 98, 182, 182, 11962, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"The cat is so very very cute\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ġcute'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(11962)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Ġcat', 'Ġis', 'Ġso', 'Ġvery', 'Ġvery', 'Ġcute']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"The cat is so very very cute\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Text' object has no attribute 'partition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m masker \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mmaskers\u001b[39m.\u001b[39mText(\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m)  \u001b[39m# non-word character masker\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Let's see how the masker partitions the text\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[39mprint\u001b[39m(masker\u001b[39m.\u001b[39;49mpartition(text))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Text' object has no attribute 'partition'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "masker = shap.maskers.Text(r\"\\W\")  # non-word character masker\n",
    "\n",
    "# Let's see how the masker partitions the text\n",
    "print(masker.partition(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = masker.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': ['Hello', '', 'world'],\n",
       " 'offset_mapping': [(0, 5), (6, 6), (7, 12)]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = shap.maskers.Text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = masker.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleTokenizer' object has no attribute 'tokenize'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mtype\u001b[39m(tk)\n\u001b[0;32m----> 2\u001b[0m tk\u001b[39m.\u001b[39;49mtokenize(text)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SimpleTokenizer' object has no attribute 'tokenize'"
     ]
    }
   ],
   "source": [
    "type(tk)\n",
    "tk.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'split_pattern']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'world', '!']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m masker \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39;49mmaskers\u001b[39m.\u001b[39;49mText(tokenizer\u001b[39m=\u001b[39;49mword_tokenize)\n",
      "File \u001b[0;32m~/anaconda3/envs/textdebias/lib/python3.9/site-packages/shap/maskers/_text.py:57\u001b[0m, in \u001b[0;36mText.__init__\u001b[0;34m(self, tokenizer, mask_token, collapse_mask_token, output_type)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_token \u001b[39m=\u001b[39m mask_token \u001b[39m# could be recomputed later in this function\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_token_id \u001b[39m=\u001b[39m mask_token \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(mask_token, \u001b[39mint\u001b[39m) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m parsed_tokenizer_dict \u001b[39m=\u001b[39m parse_prefix_suffix_for_tokenizer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer)\n\u001b[1;32m     59\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_prefix \u001b[39m=\u001b[39m parsed_tokenizer_dict[\u001b[39m'\u001b[39m\u001b[39mkeep_prefix\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_suffix \u001b[39m=\u001b[39m parsed_tokenizer_dict[\u001b[39m'\u001b[39m\u001b[39mkeep_suffix\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/textdebias/lib/python3.9/site-packages/shap/utils/transformers.py:98\u001b[0m, in \u001b[0;36mparse_prefix_suffix_for_tokenizer\u001b[0;34m(tokenizer)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mparse_prefix_suffix_for_tokenizer\u001b[39m(tokenizer):\n\u001b[1;32m     93\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" Set prefix and suffix tokens based on null tokens.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \n\u001b[1;32m     95\u001b[0m \u001b[39m    Example for distillgpt2: null_tokens=[], for BART: null_tokens = [0,2] and for MarianMT: null_tokens=[0]\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m    used to slice tokens belonging to sentence after passing through tokenizer.encode().\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m     null_tokens \u001b[39m=\u001b[39m tokenizer(\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     99\u001b[0m     keep_prefix, keep_suffix \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(null_tokens) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "masker = shap.maskers.Text(tokenizer=word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mm = shap.maskers.Text(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = mm.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'Ġworld', '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textdebias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
